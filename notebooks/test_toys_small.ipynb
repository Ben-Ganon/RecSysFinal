{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ZhcQhEIjGeN"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append('../')\n",
        "\n",
        "import collections\n",
        "import os\n",
        "import random\n",
        "from pathlib import Path\n",
        "import logging\n",
        "import shutil\n",
        "import time\n",
        "from packaging import version\n",
        "from collections import defaultdict\n",
        "\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import gzip\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn.parallel import DistributedDataParallel as DDP\n",
        "import torch.distributed as dist\n",
        "import torch.backends.cudnn as cudnn\n",
        "\n",
        "from src.param import parse_args\n",
        "from src.utils import LossMeter\n",
        "from src.dist_utils import reduce_dict\n",
        "from transformers import T5Tokenizer, T5TokenizerFast\n",
        "from src.tokenization import P5Tokenizer, P5TokenizerFast\n",
        "from src.pretrain_model import P5Pretraining\n",
        "\n",
        "_use_native_amp = False\n",
        "_use_apex = False\n",
        "\n",
        "# Check if Pytorch version >= 1.6 to switch between Native AMP and Apex\n",
        "if version.parse(torch.__version__) < version.parse(\"1.6\"):\n",
        "    from transormers.file_utils import is_apex_available\n",
        "    if is_apex_available():\n",
        "        from apex import amp\n",
        "    _use_apex = True\n",
        "else:\n",
        "    _use_native_amp = True\n",
        "    from torch.cuda.amp import autocast\n",
        "\n",
        "from src.trainer_base import TrainerBase\n",
        "\n",
        "import pickle\n",
        "\n",
        "def load_pickle(filename):\n",
        "    with open(filename, \"rb\") as f:\n",
        "        return pickle.load(f)\n",
        "\n",
        "\n",
        "def save_pickle(data, filename):\n",
        "    with open(filename, \"wb\") as f:\n",
        "        pickle.dump(data, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "import json\n",
        "\n",
        "def load_json(file_path):\n",
        "    with open(file_path, \"r\") as f:\n",
        "        return json.load(f)\n",
        "\n",
        "def ReadLineFromFile(path):\n",
        "    lines = []\n",
        "    with open(path,'r') as fd:\n",
        "        for line in fd:\n",
        "            lines.append(line.rstrip('\\n'))\n",
        "    return lines\n",
        "\n",
        "def parse(path):\n",
        "    g = gzip.open(path, 'r')\n",
        "    for l in g:\n",
        "        yield eval(l)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VANq5N0vjGeO",
        "outputId": "7baae9dc-a3a1-4d7d-c7cf-255078189e3a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['rating_loss', 'sequential_loss', 'explanation_loss', 'review_loss', 'traditional_loss']\n",
            "Process Launching at GPU 0\n",
            "{'distributed': False, 'multiGPU': True, 'fp16': True, 'train': 'sports', 'valid': 'sports', 'test': 'sports', 'batch_size': 16, 'optim': 'adamw', 'warmup_ratio': 0.05, 'lr': 0.001, 'num_workers': 4, 'clip_grad_norm': 1.0, 'losses': 'rating,sequential,explanation,review,traditional', 'backbone': 't5-small', 'output': 'snap/sports-small', 'epoch': 10, 'local_rank': 0, 'comment': '', 'train_topk': -1, 'valid_topk': -1, 'dropout': 0.1, 'tokenizer': 'p5', 'max_text_length': 512, 'do_lower_case': False, 'word_mask_rate': 0.15, 'gen_max_length': 64, 'weight_decay': 0.01, 'adam_eps': 1e-06, 'gradient_accumulation_steps': 1, 'seed': 2022, 'whole_word_embed': True, 'world_size': 1, 'LOSSES_NAME': ['rating_loss', 'sequential_loss', 'explanation_loss', 'review_loss', 'traditional_loss', 'total_loss'], 'gpu': 0, 'rank': 0}\n"
          ]
        }
      ],
      "source": [
        "class DotDict(dict):\n",
        "    def __init__(self, **kwds):\n",
        "        self.update(kwds)\n",
        "        self.__dict__ = self\n",
        "\n",
        "args = DotDict()\n",
        "\n",
        "args.distributed = False\n",
        "args.multiGPU = True\n",
        "args.fp16 = True\n",
        "args.train = \"toys\"\n",
        "args.valid = \"toys\"\n",
        "args.test = \"toys\"\n",
        "args.batch_size = 16\n",
        "args.optim = 'adamw'\n",
        "args.warmup_ratio = 0.05\n",
        "args.lr = 1e-3\n",
        "args.num_workers = 4\n",
        "args.clip_grad_norm = 1.0\n",
        "args.losses = 'rating,sequential,explanation,review,traditional'\n",
        "args.backbone = 't5-small' # small or base\n",
        "args.output = 'snap/toys-small'\n",
        "args.epoch = 10\n",
        "args.local_rank = 0\n",
        "\n",
        "args.comment = ''\n",
        "args.train_topk = -1\n",
        "args.valid_topk = -1\n",
        "args.dropout = 0.1\n",
        "\n",
        "args.tokenizer = 'p5'\n",
        "args.max_text_length = 512\n",
        "args.do_lower_case = False\n",
        "args.word_mask_rate = 0.15\n",
        "args.gen_max_length = 64\n",
        "\n",
        "args.weight_decay = 0.01\n",
        "args.adam_eps = 1e-6\n",
        "args.gradient_accumulation_steps = 1\n",
        "\n",
        "'''\n",
        "Set seeds\n",
        "'''\n",
        "args.seed = 2022\n",
        "torch.manual_seed(args.seed)\n",
        "random.seed(args.seed)\n",
        "np.random.seed(args.seed)\n",
        "\n",
        "'''\n",
        "Whole word embedding\n",
        "'''\n",
        "args.whole_word_embed = True\n",
        "\n",
        "cudnn.benchmark = True\n",
        "ngpus_per_node = torch.cuda.device_count()\n",
        "args.world_size = ngpus_per_node\n",
        "\n",
        "LOSSES_NAME = [f'{name}_loss' for name in args.losses.split(',')]\n",
        "if args.local_rank in [0, -1]:\n",
        "    print(LOSSES_NAME)\n",
        "LOSSES_NAME.append('total_loss') # total loss\n",
        "\n",
        "args.LOSSES_NAME = LOSSES_NAME\n",
        "\n",
        "gpu = 0 # Change GPU ID\n",
        "args.gpu = gpu\n",
        "args.rank = gpu\n",
        "print(f'Process Launching at GPU {gpu}')\n",
        "\n",
        "torch.cuda.set_device('cuda:{}'.format(gpu))\n",
        "\n",
        "comments = []\n",
        "dsets = []\n",
        "if 'toys' in args.train:\n",
        "    dsets.append('toys')\n",
        "if 'beauty' in args.train:\n",
        "    dsets.append('beauty')\n",
        "if 'sports' in args.train:\n",
        "    dsets.append('sports')\n",
        "comments.append(''.join(dsets))\n",
        "if args.backbone:\n",
        "    comments.append(args.backbone)\n",
        "comments.append(''.join(args.losses.split(',')))\n",
        "if args.comment != '':\n",
        "    comments.append(args.comment)\n",
        "comment = '_'.join(comments)\n",
        "\n",
        "if args.local_rank in [0, -1]:\n",
        "    print(args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eNvrWIarjGeP"
      },
      "outputs": [],
      "source": [
        "def create_config(args):\n",
        "    from transformers import T5Config, BartConfig\n",
        "\n",
        "    if 't5' in args.backbone:\n",
        "        config_class = T5Config\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "    config = config_class.from_pretrained(args.backbone)\n",
        "    config.dropout_rate = args.dropout\n",
        "    config.dropout = args.dropout\n",
        "    config.attention_dropout = args.dropout\n",
        "    config.activation_dropout = args.dropout\n",
        "    config.losses = args.losses\n",
        "\n",
        "    return config\n",
        "\n",
        "\n",
        "def create_tokenizer(args):\n",
        "    from transformers import T5Tokenizer, T5TokenizerFast\n",
        "    from src.tokenization import P5Tokenizer, P5TokenizerFast\n",
        "\n",
        "    if 'p5' in args.tokenizer:\n",
        "        tokenizer_class = P5Tokenizer\n",
        "\n",
        "    tokenizer_name = args.backbone\n",
        "\n",
        "    tokenizer = tokenizer_class.from_pretrained(\n",
        "        tokenizer_name,\n",
        "        max_length=args.max_text_length,\n",
        "        do_lower_case=args.do_lower_case,\n",
        "    )\n",
        "\n",
        "    print(tokenizer_class, tokenizer_name)\n",
        "\n",
        "    return tokenizer\n",
        "\n",
        "\n",
        "def create_model(model_class, config=None):\n",
        "    print(f'Building Model at GPU {args.gpu}')\n",
        "\n",
        "    model_name = args.backbone\n",
        "\n",
        "    model = model_class.from_pretrained(\n",
        "        model_name,\n",
        "        config=config\n",
        "    )\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mTUkbohSjGeQ",
        "outputId": "ecb269c0-e74b-46eb-cad9-34037e9d009e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'src.tokenization.P5Tokenizer'> t5-small\n",
            "Building Model at GPU 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of P5Pretraining were not initialized from the model checkpoint at t5-small and are newly initialized: ['encoder.whole_word_embeddings.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "config = create_config(args)\n",
        "\n",
        "if args.tokenizer is None:\n",
        "    args.tokenizer = args.backbone\n",
        "\n",
        "tokenizer = create_tokenizer(args)\n",
        "\n",
        "model_class = P5Pretraining\n",
        "model = create_model(model_class, config)\n",
        "\n",
        "model = model.cuda()\n",
        "\n",
        "if 'p5' in args.tokenizer:\n",
        "    model.resize_token_embeddings(tokenizer.vocab_size)\n",
        "\n",
        "model.tokenizer = tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWoMzmEZjGeQ",
        "tags": []
      },
      "source": [
        "#### Load Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GwOpE_nLjGeR",
        "outputId": "483a322a-15ad-4284-e230-4dc41005d968"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model loaded from  ../snap/sports-small.pth\n",
            "<All keys matched successfully>\n"
          ]
        }
      ],
      "source": [
        "args.load = \"../snap/toys-small.pth\"\n",
        "\n",
        "# Load Checkpoint\n",
        "from src.utils import load_state_dict, LossMeter, set_global_logging_level\n",
        "from pprint import pprint\n",
        "\n",
        "def load_checkpoint(ckpt_path):\n",
        "    state_dict = load_state_dict(ckpt_path, 'cpu')\n",
        "    results = model.load_state_dict(state_dict, strict=False)\n",
        "    print('Model loaded from ', ckpt_path)\n",
        "    pprint(results)\n",
        "\n",
        "ckpt_path = args.load\n",
        "load_checkpoint(ckpt_path)\n",
        "\n",
        "from src.all_amazon_templates import all_tasks as task_templates"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8H7XtxnjGeR"
      },
      "source": [
        "#### Check Test Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zLNhbgX3jGeR"
      },
      "outputs": [],
      "source": [
        "path = os.getcwd()\n",
        "data_splits = load_pickle(f'{path}/data/toys/rating_splits_augmented.pkl')\n",
        "# data_splits = load_pickle('../data/toys/rating_splits_augmented.pkl')\n",
        "test_review_data = data_splits['test']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VhHBRRnOjGeS",
        "outputId": "3cbf88ab-0ffb-47be-f970-7e30d5e9789c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "29633"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(test_review_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2dY_71YjjGeS",
        "outputId": "93dc33a7-062f-43c6-e400-1fc8582e911c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'reviewerID': 'AAAWJ6LW9WMOO',\n",
              " 'asin': '1881509818',\n",
              " 'reviewerName': 'Material Man',\n",
              " 'helpful': [0, 0],\n",
              " 'reviewText': 'I purchased this thinking maybe I need a special tool to easily pop off my base plates for my magazines, but it does the same as a regular punch tool. Glock mags are a pain to get the base plates off.  The tool does not really make a difference.',\n",
              " 'overall': 4.0,\n",
              " 'summary': 'Ok,tool does what a regular punch does.',\n",
              " 'unixReviewTime': 1366675200,\n",
              " 'reviewTime': '04 23, 2013'}"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_review_data[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z-nKOPlwjGeS",
        "outputId": "660a556f-71ca-4c51-fae4-08878d8059e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "35598\n",
            "18357\n"
          ]
        }
      ],
      "source": [
        "path = os.getcwd()\n",
        "data_maps = load_json(os.path.join(f'{path}/data', 'toys', 'datamaps.json'))\n",
        "print(len(data_maps['user2id'])) # number of users\n",
        "print(len(data_maps['item2id'])) # number of items"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y3q3LKwKjGeS"
      },
      "source": [
        "### Test P5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WcXoH14ZjGeS"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader, Dataset, Sampler\n",
        "from src.pretrain_data import get_loader\n",
        "from evaluate.utils import rouge_score, bleu_score, unique_sentence_percent, root_mean_square_error, mean_absolute_error, feature_detect, feature_matching_ratio, feature_coverage_ratio, feature_diversity\n",
        "from evaluate.metrics4rec import evaluate_all"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KhMLPiIGjGeT"
      },
      "source": [
        "#### Evaluation - Rating"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "editable": true,
        "id": "jHKDGtVWjGeT",
        "outputId": "182803de-82e0-4e97-adc3-1b07a95ad8ed",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data sources:  ['sports']\n",
            "compute_datum_info\n",
            "1853\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1853it [02:10, 14.18it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RMSE  1.0529\n",
            "MAE  0.6693\n"
          ]
        }
      ],
      "source": [
        "test_task_list = {'rating': ['1-10'] # or '1-6'\n",
        "}\n",
        "test_sample_numbers = {'rating': 1, 'sequential': (1, 1, 1), 'explanation': 1, 'review': 1, 'traditional': (1, 1)}\n",
        "\n",
        "zeroshot_test_loader = get_loader(\n",
        "        args,\n",
        "        test_task_list,\n",
        "        test_sample_numbers,\n",
        "        split=args.test,\n",
        "        mode='test',\n",
        "        batch_size=args.batch_size,\n",
        "        workers=args.num_workers,\n",
        "        distributed=args.distributed\n",
        ")\n",
        "print(len(zeroshot_test_loader))\n",
        "\n",
        "gt_ratings = []\n",
        "pred_ratings = []\n",
        "for i, batch in tqdm(enumerate(zeroshot_test_loader)):\n",
        "    with torch.no_grad():\n",
        "        results = model.generate_step(batch)\n",
        "        gt_ratings.extend(batch['target_text'])\n",
        "        pred_ratings.extend(results)\n",
        "\n",
        "predicted_rating = [(float(r), float(p)) for (r, p) in zip(gt_ratings, pred_ratings) if p in [str(i/10.0) for i in list(range(10, 50))]]\n",
        "RMSE = root_mean_square_error(predicted_rating, 5.0, 1.0)\n",
        "print('RMSE {:7.4f}'.format(RMSE))\n",
        "MAE = mean_absolute_error(predicted_rating, 5.0, 1.0)\n",
        "print('MAE {:7.4f}'.format(MAE))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gGxZHVUejGeT",
        "outputId": "785e1d3e-00dd-45d3-e0c3-3fca6b3b924b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data sources:  ['sports']\n",
            "compute_datum_info\n",
            "1853\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1853it [02:08, 14.43it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RMSE  1.0594\n",
            "MAE  0.6639\n"
          ]
        }
      ],
      "source": [
        "test_task_list = {'rating': ['1-6'] # or '1-10'\n",
        "}\n",
        "test_sample_numbers = {'rating': 1, 'sequential': (1, 1, 1), 'explanation': 1, 'review': 1, 'traditional': (1, 1)}\n",
        "\n",
        "zeroshot_test_loader = get_loader(\n",
        "        args,\n",
        "        test_task_list,\n",
        "        test_sample_numbers,\n",
        "        split=args.test,\n",
        "        mode='test',\n",
        "        batch_size=args.batch_size,\n",
        "        workers=args.num_workers,\n",
        "        distributed=args.distributed\n",
        ")\n",
        "print(len(zeroshot_test_loader))\n",
        "\n",
        "gt_ratings = []\n",
        "pred_ratings = []\n",
        "for i, batch in tqdm(enumerate(zeroshot_test_loader)):\n",
        "    with torch.no_grad():\n",
        "        results = model.generate_step(batch)\n",
        "        gt_ratings.extend(batch['target_text'])\n",
        "        pred_ratings.extend(results)\n",
        "\n",
        "predicted_rating = [(float(r), float(p)) for (r, p) in zip(gt_ratings, pred_ratings) if p in [str(i/10.0) for i in list(range(10, 50))]]\n",
        "RMSE = root_mean_square_error(predicted_rating, 5.0, 1.0)\n",
        "print('RMSE {:7.4f}'.format(RMSE))\n",
        "MAE = mean_absolute_error(predicted_rating, 5.0, 1.0)\n",
        "print('MAE {:7.4f}'.format(MAE))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g28398MOjGeT",
        "tags": []
      },
      "source": [
        "#### Evaluation - Sequential"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Yytt4ltjGeT",
        "outputId": "af2197ce-487e-4c42-a003-7f4720e167d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data sources:  ['sports']\n",
            "compute_datum_info\n",
            "2225\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/noagoren/.conda/envs/recsys/lib/python3.9/site-packages/transformers/generation_utils.py:1632: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  next_indices = next_tokens // vocab_size\n",
            "2225it [23:35,  1.57it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "NDCG@5\tRec@5\tHits@5\tPrec@5\tMAP@5\tMRR@5\n",
            "0.0160\t0.0259\t0.0259\t0.0052\t0.0128\t0.0128\n",
            "\n",
            "NDCG@10\tRec@10\tHits@10\tPrec@10\tMAP@10\tMRR@10\n",
            "0.0189\t0.0347\t0.0347\t0.0035\t0.0139\t0.0139\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "('\\nNDCG@10\\tRec@10\\tHits@10\\tPrec@10\\tMAP@10\\tMRR@10\\n0.0189\\t0.0347\\t0.0347\\t0.0035\\t0.0139\\t0.0139',\n",
              " {'ndcg': 0.018879018021782917,\n",
              "  'map': 0.013947368186370379,\n",
              "  'recall': 0.03472105174448003,\n",
              "  'precision': 0.0034721051744479254,\n",
              "  'mrr': 0.013947368186370379,\n",
              "  'hit': 0.03472105174448003})"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_task_list = {'sequential': ['2-13'] # or '2-3'\n",
        "}\n",
        "test_sample_numbers = {'rating': 1, 'sequential': (1, 1, 1), 'explanation': 1, 'review': 1, 'traditional': (1, 1)}\n",
        "\n",
        "zeroshot_test_loader = get_loader(\n",
        "        args,\n",
        "        test_task_list,\n",
        "        test_sample_numbers,\n",
        "        split=args.test,\n",
        "        mode='test',\n",
        "        batch_size=args.batch_size,\n",
        "        workers=args.num_workers,\n",
        "        distributed=args.distributed\n",
        ")\n",
        "print(len(zeroshot_test_loader))\n",
        "\n",
        "all_info = []\n",
        "for i, batch in tqdm(enumerate(zeroshot_test_loader)):\n",
        "    with torch.no_grad():\n",
        "        results = model.generate_step(batch)\n",
        "        beam_outputs = model.generate(\n",
        "                batch['input_ids'].to('cuda'),\n",
        "                max_length=50,\n",
        "                num_beams=20,\n",
        "                no_repeat_ngram_size=0,\n",
        "                num_return_sequences=20,\n",
        "                early_stopping=True\n",
        "        )\n",
        "        generated_sents = model.tokenizer.batch_decode(beam_outputs, skip_special_tokens=True)\n",
        "        for j, item in enumerate(zip(results, batch['target_text'], batch['source_text'])):\n",
        "            new_info = {}\n",
        "            new_info['target_item'] = item[1]\n",
        "            new_info['gen_item_list'] = generated_sents[j*20: (j+1)*20]\n",
        "            all_info.append(new_info)\n",
        "\n",
        "gt = {}\n",
        "ui_scores = {}\n",
        "for i, info in enumerate(all_info):\n",
        "    gt[i] = [int(info['target_item'])]\n",
        "    pred_dict = {}\n",
        "    for j in range(len(info['gen_item_list'])):\n",
        "        try:\n",
        "            pred_dict[int(info['gen_item_list'][j])] = -(j+1)\n",
        "        except:\n",
        "            pass\n",
        "    ui_scores[i] = pred_dict\n",
        "\n",
        "evaluate_all(ui_scores, gt, 5)\n",
        "evaluate_all(ui_scores, gt, 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fbxoKVVBjGeU"
      },
      "outputs": [],
      "source": [
        "test_task_list = {'sequential': ['2-3'] # or '2-13'\n",
        "}\n",
        "test_sample_numbers = {'rating': 1, 'sequential': (1, 1, 1), 'explanation': 1, 'review': 1, 'traditional': (1, 1)}\n",
        "\n",
        "zeroshot_test_loader = get_loader(\n",
        "        args,\n",
        "        test_task_list,\n",
        "        test_sample_numbers,\n",
        "        split=args.test,\n",
        "        mode='test',\n",
        "        batch_size=args.batch_size,\n",
        "        workers=args.num_workers,\n",
        "        distributed=args.distributed\n",
        ")\n",
        "print(len(zeroshot_test_loader))\n",
        "\n",
        "all_info = []\n",
        "for i, batch in tqdm(enumerate(zeroshot_test_loader)):\n",
        "    with torch.no_grad():\n",
        "        results = model.generate_step(batch)\n",
        "        beam_outputs = model.generate(\n",
        "                batch['input_ids'].to('cuda'),\n",
        "                max_length=50,\n",
        "                num_beams=20,\n",
        "                no_repeat_ngram_size=0,\n",
        "                num_return_sequences=20,\n",
        "                early_stopping=True\n",
        "        )\n",
        "        generated_sents = model.tokenizer.batch_decode(beam_outputs, skip_special_tokens=True)\n",
        "        for j, item in enumerate(zip(results, batch['target_text'], batch['source_text'])):\n",
        "            new_info = {}\n",
        "            new_info['target_item'] = item[1]\n",
        "            new_info['gen_item_list'] = generated_sents[j*20: (j+1)*20]\n",
        "            all_info.append(new_info)\n",
        "\n",
        "gt = {}\n",
        "ui_scores = {}\n",
        "for i, info in enumerate(all_info):\n",
        "    gt[i] = [int(info['target_item'])]\n",
        "    pred_dict = {}\n",
        "    for j in range(len(info['gen_item_list'])):\n",
        "        try:\n",
        "            pred_dict[int(info['gen_item_list'][j])] = -(j+1)\n",
        "        except:\n",
        "            pass\n",
        "    ui_scores[i] = pred_dict\n",
        "\n",
        "evaluate_all(ui_scores, gt, 5)\n",
        "evaluate_all(ui_scores, gt, 10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "decW2XXQjGeU"
      },
      "source": [
        "#### Evaluation - Explanation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZsDSFngQjGeU"
      },
      "outputs": [],
      "source": [
        "test_task_list = {'explanation': ['3-12'] # or '3-9' or '3-3'\n",
        "}\n",
        "test_sample_numbers = {'rating': 1, 'sequential': (1, 1, 1), 'explanation': 1, 'review': 1, 'traditional': (1, 1)}\n",
        "\n",
        "zeroshot_test_loader = get_loader(\n",
        "        args,\n",
        "        test_task_list,\n",
        "        test_sample_numbers,\n",
        "        split=args.test,\n",
        "        mode='test',\n",
        "        batch_size=args.batch_size,\n",
        "        workers=args.num_workers,\n",
        "        distributed=args.distributed\n",
        ")\n",
        "print(len(zeroshot_test_loader))\n",
        "\n",
        "tokens_predict = []\n",
        "tokens_test = []\n",
        "for i, batch in tqdm(enumerate(zeroshot_test_loader)):\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "                batch['input_ids'].to('cuda'),\n",
        "                min_length=9,\n",
        "                num_beams=12,\n",
        "                num_return_sequences=1,\n",
        "                num_beam_groups=3,\n",
        "                repetition_penalty=0.7\n",
        "        )\n",
        "        results = model.tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "        tokens_predict.extend(results)\n",
        "        tokens_test.extend(batch['target_text'])\n",
        "\n",
        "new_tokens_predict = [l.split() for l in tokens_predict]\n",
        "new_tokens_test = [ll.split() for ll in tokens_test]\n",
        "BLEU1 = bleu_score(new_tokens_test, new_tokens_predict, n_gram=1, smooth=False)\n",
        "BLEU4 = bleu_score(new_tokens_test, new_tokens_predict, n_gram=4, smooth=False)\n",
        "ROUGE = rouge_score(tokens_test, tokens_predict)\n",
        "\n",
        "print('BLEU-1 {:7.4f}'.format(BLEU1))\n",
        "print('BLEU-4 {:7.4f}'.format(BLEU4))\n",
        "for (k, v) in ROUGE.items():\n",
        "    print('{} {:7.4f}'.format(k, v))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uwkb-lHXjGeU"
      },
      "outputs": [],
      "source": [
        "test_task_list = {'explanation': ['3-9'] # or '3-12' or '3-3'\n",
        "}\n",
        "test_sample_numbers = {'rating': 1, 'sequential': (1, 1, 1), 'explanation': 1, 'review': 1, 'traditional': (1, 1)}\n",
        "\n",
        "zeroshot_test_loader = get_loader(\n",
        "        args,\n",
        "        test_task_list,\n",
        "        test_sample_numbers,\n",
        "        split=args.test,\n",
        "        mode='test',\n",
        "        batch_size=args.batch_size,\n",
        "        workers=args.num_workers,\n",
        "        distributed=args.distributed\n",
        ")\n",
        "print(len(zeroshot_test_loader))\n",
        "\n",
        "tokens_predict = []\n",
        "tokens_test = []\n",
        "for i, batch in tqdm(enumerate(zeroshot_test_loader)):\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "                batch['input_ids'].to('cuda'),\n",
        "                min_length=10,\n",
        "                num_beams=12,\n",
        "                num_return_sequences=1,\n",
        "                num_beam_groups=3\n",
        "        )\n",
        "        results = model.tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "        tokens_predict.extend(results)\n",
        "        tokens_test.extend(batch['target_text'])\n",
        "\n",
        "new_tokens_predict = [l.split() for l in tokens_predict]\n",
        "new_tokens_test = [ll.split() for ll in tokens_test]\n",
        "BLEU1 = bleu_score(new_tokens_test, new_tokens_predict, n_gram=1, smooth=False)\n",
        "BLEU4 = bleu_score(new_tokens_test, new_tokens_predict, n_gram=4, smooth=False)\n",
        "ROUGE = rouge_score(tokens_test, tokens_predict)\n",
        "\n",
        "print('BLEU-1 {:7.4f}'.format(BLEU1))\n",
        "print('BLEU-4 {:7.4f}'.format(BLEU4))\n",
        "for (k, v) in ROUGE.items():\n",
        "    print('{} {:7.4f}'.format(k, v))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jq6na-XijGeU"
      },
      "outputs": [],
      "source": [
        "test_task_list = {'explanation': ['3-3'] # or '3-12' or '3-9'\n",
        "}\n",
        "test_sample_numbers = {'rating': 1, 'sequential': (1, 1, 1), 'explanation': 1, 'review': 1, 'traditional': (1, 1)}\n",
        "\n",
        "zeroshot_test_loader = get_loader(\n",
        "        args,\n",
        "        test_task_list,\n",
        "        test_sample_numbers,\n",
        "        split=args.test,\n",
        "        mode='test',\n",
        "        batch_size=args.batch_size,\n",
        "        workers=args.num_workers,\n",
        "        distributed=args.distributed\n",
        ")\n",
        "print(len(zeroshot_test_loader))\n",
        "\n",
        "tokens_predict = []\n",
        "tokens_test = []\n",
        "for i, batch in tqdm(enumerate(zeroshot_test_loader)):\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "                batch['input_ids'].to('cuda'),\n",
        "                min_length=10\n",
        "        )\n",
        "        results = model.tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "        tokens_predict.extend(results)\n",
        "        tokens_test.extend(batch['target_text'])\n",
        "\n",
        "new_tokens_predict = [l.split() for l in tokens_predict]\n",
        "new_tokens_test = [ll.split() for ll in tokens_test]\n",
        "BLEU1 = bleu_score(new_tokens_test, new_tokens_predict, n_gram=1, smooth=False)\n",
        "BLEU4 = bleu_score(new_tokens_test, new_tokens_predict, n_gram=4, smooth=False)\n",
        "ROUGE = rouge_score(tokens_test, tokens_predict)\n",
        "\n",
        "print('BLEU-1 {:7.4f}'.format(BLEU1))\n",
        "print('BLEU-4 {:7.4f}'.format(BLEU4))\n",
        "for (k, v) in ROUGE.items():\n",
        "    print('{} {:7.4f}'.format(k, v))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vkg22-nFjGeV",
        "tags": []
      },
      "source": [
        "#### Evaluation - Review\n",
        "\n",
        "Since T0 & GPT-2 checkpoints hosted on Hugging Face platform are slow to conduct inference, we only perform evaluation on the first 800 instances for prompts in Task Family 4."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yJTlBNVmjGeV"
      },
      "outputs": [],
      "source": [
        "test_task_list = {'review': ['4-4'] # or '4-2'\n",
        "}\n",
        "test_sample_numbers = {'rating': 1, 'sequential': (1, 1, 1), 'explanation': 1, 'review': 1, 'traditional': (1, 1)}\n",
        "\n",
        "zeroshot_test_loader = get_loader(\n",
        "        args,\n",
        "        test_task_list,\n",
        "        test_sample_numbers,\n",
        "        split=args.test,\n",
        "        mode='test',\n",
        "        batch_size=args.batch_size,\n",
        "        workers=args.num_workers,\n",
        "        distributed=args.distributed\n",
        ")\n",
        "print(len(zeroshot_test_loader))\n",
        "\n",
        "gt_ratings = []\n",
        "pred_ratings = []\n",
        "for i, batch in tqdm(enumerate(zeroshot_test_loader)):\n",
        "    if i > 50:\n",
        "        break\n",
        "    with torch.no_grad():\n",
        "        results = model.generate_step(batch)\n",
        "        gt_ratings.extend(batch['target_text'])\n",
        "        pred_ratings.extend(results)\n",
        "\n",
        "predicted_rating = [(float(r), round(float(p))) for (r, p) in zip(gt_ratings, pred_ratings)]\n",
        "RMSE = root_mean_square_error(predicted_rating, 5.0, 1.0)\n",
        "print('RMSE {:7.4f}'.format(RMSE))\n",
        "MAE = mean_absolute_error(predicted_rating, 5.0, 1.0)\n",
        "print('MAE {:7.4f}'.format(MAE))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OscCL1lYjGeV"
      },
      "outputs": [],
      "source": [
        "test_task_list = {'review': ['4-2'] # or '4-4'\n",
        "}\n",
        "test_sample_numbers = {'rating': 1, 'sequential': (1, 1, 1), 'explanation': 1, 'review': 1, 'traditional': (1, 1)}\n",
        "\n",
        "zeroshot_test_loader = get_loader(\n",
        "        args,\n",
        "        test_task_list,\n",
        "        test_sample_numbers,\n",
        "        split=args.test,\n",
        "        mode='test',\n",
        "        batch_size=args.batch_size,\n",
        "        workers=args.num_workers,\n",
        "        distributed=args.distributed\n",
        ")\n",
        "print(len(zeroshot_test_loader))\n",
        "\n",
        "gt_ratings = []\n",
        "pred_ratings = []\n",
        "for i, batch in tqdm(enumerate(zeroshot_test_loader)):\n",
        "    if i > 50:\n",
        "        break\n",
        "    with torch.no_grad():\n",
        "        results = model.generate_step(batch)\n",
        "        gt_ratings.extend(batch['target_text'])\n",
        "        pred_ratings.extend(results)\n",
        "\n",
        "predicted_rating = [(float(r), round(float(p))) for (r, p) in zip(gt_ratings, pred_ratings)]\n",
        "RMSE = root_mean_square_error(predicted_rating, 5.0, 1.0)\n",
        "print('RMSE {:7.4f}'.format(RMSE))\n",
        "MAE = mean_absolute_error(predicted_rating, 5.0, 1.0)\n",
        "print('MAE {:7.4f}'.format(MAE))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TGAu5XzbjGeV"
      },
      "outputs": [],
      "source": [
        "test_task_list = {'review': ['4-1']\n",
        "}\n",
        "test_sample_numbers = {'rating': 1, 'sequential': (1, 1, 1), 'explanation': 1, 'review': 1, 'traditional': (1, 1)}\n",
        "\n",
        "zeroshot_test_loader = get_loader(\n",
        "        args,\n",
        "        test_task_list,\n",
        "        test_sample_numbers,\n",
        "        split=args.test,\n",
        "        mode='test',\n",
        "        batch_size=args.batch_size,\n",
        "        workers=args.num_workers,\n",
        "        distributed=args.distributed\n",
        ")\n",
        "print(len(zeroshot_test_loader))\n",
        "\n",
        "tokens_predict = []\n",
        "tokens_test = []\n",
        "for i, batch in tqdm(enumerate(zeroshot_test_loader)):\n",
        "    if i > 50:\n",
        "        break\n",
        "    with torch.no_grad():\n",
        "        results = model.generate_step(batch)\n",
        "        tokens_predict.extend(results)\n",
        "        tokens_test.extend(batch['target_text'])\n",
        "\n",
        "new_tokens_predict = [l.split() for l in tokens_predict]\n",
        "new_tokens_test = [ll.split() for ll in tokens_test]\n",
        "BLEU2 = bleu_score(new_tokens_test, new_tokens_predict, n_gram=2, smooth=False)\n",
        "ROUGE = rouge_score(tokens_test, tokens_predict)\n",
        "\n",
        "print('BLEU-2 {:7.4f}'.format(BLEU2))\n",
        "for (k, v) in ROUGE.items():\n",
        "    print('{} {:7.4f}'.format(k, v))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dc3stVVwjGeV",
        "tags": []
      },
      "source": [
        "#### Evaluation - Traditional"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d8wFHiOGjGeV"
      },
      "outputs": [],
      "source": [
        "test_task_list = {'traditional': ['5-8']  # or '5-5'\n",
        "}\n",
        "test_sample_numbers = {'rating': 1, 'sequential': (1, 1, 1), 'explanation': 1, 'review': 1, 'traditional': (1, 1)}\n",
        "\n",
        "zeroshot_test_loader = get_loader(\n",
        "        args,\n",
        "        test_task_list,\n",
        "        test_sample_numbers,\n",
        "        split=args.test,\n",
        "        mode='test',\n",
        "        batch_size=args.batch_size,\n",
        "        workers=args.num_workers,\n",
        "        distributed=args.distributed\n",
        ")\n",
        "print(len(zeroshot_test_loader))\n",
        "\n",
        "all_info = []\n",
        "for i, batch in tqdm(enumerate(zeroshot_test_loader)):\n",
        "    with torch.no_grad():\n",
        "        results = model.generate_step(batch)\n",
        "        beam_outputs = model.generate(\n",
        "                batch['input_ids'].to('cuda'),\n",
        "                max_length=50,\n",
        "                num_beams=20,\n",
        "                no_repeat_ngram_size=0,\n",
        "                num_return_sequences=20,\n",
        "                early_stopping=True\n",
        "        )\n",
        "        generated_sents = model.tokenizer.batch_decode(beam_outputs, skip_special_tokens=True)\n",
        "        for j, item in enumerate(zip(results, batch['target_text'], batch['source_text'])):\n",
        "            new_info = {}\n",
        "            new_info['target_item'] = item[1]\n",
        "            new_info['gen_item_list'] = generated_sents[j*20: (j+1)*20]\n",
        "            all_info.append(new_info)\n",
        "\n",
        "gt = {}\n",
        "ui_scores = {}\n",
        "for i, info in enumerate(all_info):\n",
        "    gt[i] = [int(info['target_item'])]\n",
        "    pred_dict = {}\n",
        "    for j in range(len(info['gen_item_list'])):\n",
        "        try:\n",
        "            pred_dict[int(info['gen_item_list'][j])] = -(j+1)\n",
        "        except:\n",
        "            pass\n",
        "    ui_scores[i] = pred_dict\n",
        "\n",
        "evaluate_all(ui_scores, gt, 1)\n",
        "evaluate_all(ui_scores, gt, 5)\n",
        "evaluate_all(ui_scores, gt, 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pQUuGjsjjGeW"
      },
      "outputs": [],
      "source": [
        "test_task_list = {'traditional': ['5-5']  # or '5-8'\n",
        "}\n",
        "test_sample_numbers = {'rating': 1, 'sequential': (1, 1, 1), 'explanation': 1, 'review': 1, 'traditional': (1, 1)}\n",
        "\n",
        "zeroshot_test_loader = get_loader(\n",
        "        args,\n",
        "        test_task_list,\n",
        "        test_sample_numbers,\n",
        "        split=args.test,\n",
        "        mode='test',\n",
        "        batch_size=args.batch_size,\n",
        "        workers=args.num_workers,\n",
        "        distributed=args.distributed\n",
        ")\n",
        "print(len(zeroshot_test_loader))\n",
        "\n",
        "all_info = []\n",
        "for i, batch in tqdm(enumerate(zeroshot_test_loader)):\n",
        "    with torch.no_grad():\n",
        "        results = model.generate_step(batch)\n",
        "        beam_outputs = model.generate(\n",
        "                batch['input_ids'].to('cuda'),\n",
        "                max_length=50,\n",
        "                num_beams=20,\n",
        "                no_repeat_ngram_size=0,\n",
        "                num_return_sequences=20,\n",
        "                early_stopping=True\n",
        "        )\n",
        "        generated_sents = model.tokenizer.batch_decode(beam_outputs, skip_special_tokens=True)\n",
        "        for j, item in enumerate(zip(results, batch['target_text'], batch['source_text'])):\n",
        "            new_info = {}\n",
        "            new_info['target_item'] = item[1]\n",
        "            new_info['gen_item_list'] = generated_sents[j*20: (j+1)*20]\n",
        "            all_info.append(new_info)\n",
        "\n",
        "gt = {}\n",
        "ui_scores = {}\n",
        "for i, info in enumerate(all_info):\n",
        "    gt[i] = [int(info['target_item'])]\n",
        "    pred_dict = {}\n",
        "    for j in range(len(info['gen_item_list'])):\n",
        "        try:\n",
        "            pred_dict[int(info['gen_item_list'][j])] = -(j+1)\n",
        "        except:\n",
        "            pass\n",
        "    ui_scores[i] = pred_dict\n",
        "\n",
        "evaluate_all(ui_scores, gt, 1)\n",
        "evaluate_all(ui_scores, gt, 5)\n",
        "evaluate_all(ui_scores, gt, 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K2V1Anu7CrD3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7pNmqLmqCrD4"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iz9fgP3BCrD4"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.21"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}